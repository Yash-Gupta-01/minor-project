{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae28526",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e8c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01aa486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77ffcbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a987a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pertinent libraries\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# [Keras Models]\n",
    "# import the Keras implementations of VGG16, VGG19, InceptionV3 and Xception models\n",
    "# the model used here is VGG16\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from keras.applications import vgg16\n",
    "from keras import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Function to create dataset (you can place this here or in a separate cell)\n",
    "def create_dataset(directory, batch_size, image_size=(224, 224)):\n",
    "    return image_dataset_from_directory(\n",
    "        directory,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df44818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e163c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "class F1Score(Metric):\n",
    "    def __init__(self, num_classes, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', shape=(num_classes,))\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros', shape=(num_classes,))\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros', shape=(num_classes,))\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.one_hot(tf.argmax(y_true, axis=1), self.num_classes)\n",
    "        y_pred = tf.one_hot(tf.argmax(y_pred, axis=1), self.num_classes)\n",
    "        \n",
    "        self.true_positives.assign_add(tf.reduce_sum(y_true * y_pred, axis=0))\n",
    "        self.false_positives.assign_add(tf.reduce_sum((1 - y_true) * y_pred, axis=0))\n",
    "        self.false_negatives.assign_add(tf.reduce_sum(y_true * (1 - y_pred), axis=0))\n",
    "\n",
    "    def result(self):\n",
    "        precision = tf.reduce_sum(self.true_positives) / (tf.reduce_sum(self.true_positives) + tf.reduce_sum(self.false_positives) + tf.keras.backend.epsilon())\n",
    "        recall = tf.reduce_sum(self.true_positives) / (tf.reduce_sum(self.true_positives) + tf.reduce_sum(self.false_negatives) + tf.keras.backend.epsilon())\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "        self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
    "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5d39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354e8e3",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7a0e0",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91907696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 600 image(s) found.\n",
      "Output directory set to c:\\Users\\BPIT\\Desktop\\minor-project-main\\MinorProject\\archive/train\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x204B4410D90>: 100%|██████████| 10000/10000 [00:16<00:00, 591.73 Samples/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10000 augmented images.\n"
     ]
    }
   ],
   "source": [
    "import Augmentor\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Create the relative path\n",
    "relative_path = os.path.join(current_dir, \"archive/train\")\n",
    "\n",
    "# Create an Augmentor pipeline\n",
    "p = Augmentor.Pipeline(relative_path)\n",
    "\n",
    "# Add operations to the pipeline\n",
    "p.rotate90(probability=0.7)\n",
    "p.flip_left_right(probability=0.5)\n",
    "p.flip_top_bottom(probability=0.5)\n",
    "p.random_brightness(probability=0.3, min_factor=0.8, max_factor=1.2)\n",
    "p.random_contrast(probability=0.3, min_factor=0.8, max_factor=1.2)\n",
    "p.random_distortion(probability=0.3, grid_width=4, grid_height=4, magnitude=8)\n",
    "\n",
    "# Calculate the number of original images\n",
    "original_image_count = len(p.augmentor_images)\n",
    "\n",
    "# Calculate how many times we need to multiply the original dataset\n",
    "multiplication_factor = 10000 // original_image_count\n",
    "\n",
    "# Generate augmented images\n",
    "p.sample(10000)\n",
    "\n",
    "print(f\"Generated {10000} augmented images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4057710",
   "metadata": {},
   "source": [
    "## Bilateral Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "553cf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bilateral_filtering(directory):\n",
    "    for filename in os.listdir(directory):       \n",
    "        # load the image\n",
    "        img = cv2.imread(os.path.join(directory, filename))\n",
    "\n",
    "        # apply bilateral filtering\n",
    "        filtered_img = cv2.bilateralFilter(img, 6, 75,1475)\n",
    "\n",
    "        # save the filtered image\n",
    "        cv2.imwrite(os.path.join(directory,filename), filtered_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b30dc",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66616e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"archive\\valid\\Positive\"\n",
    "Bilateral_filtering(directory)\n",
    "directory = r\"archive\\valid\\Positive\"\n",
    "Bilateral_filtering(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54e5245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"archive\\valid\\Negative\"\n",
    "Bilateral_filtering(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cb4c2",
   "metadata": {},
   "source": [
    "### Val and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40c0224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"archive\\valid\\Positive\"\n",
    "Bilateral_filtering(directory)\n",
    "\n",
    "directory = r\"archive\\valid\\Negative\"\n",
    "Bilateral_filtering(directory)\n",
    "\n",
    "directory = r\"archive\\valid\\Positive\"\n",
    "Bilateral_filtering(directory)\n",
    "\n",
    "directory = r\"archive\\valid\\Negative\"\n",
    "Bilateral_filtering(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc715e89",
   "metadata": {},
   "source": [
    "## Grayscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da7c528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grayscaling(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            image = cv2.imread(filepath)\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            cv2.imwrite(filepath, gray_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e38f7d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26a7a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"archive\\valid\\Positive\"\n",
    "Grayscaling(directory)\n",
    "directory = r\"archive\\valid\\Negative\"\n",
    "Grayscaling(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74726b06",
   "metadata": {},
   "source": [
    "### Val and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef90f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"archive\\valid\\Positive\"\n",
    "Grayscaling(directory)\n",
    "directory = r\"archive\\valid\\Negative\"\n",
    "Grayscaling(directory)\n",
    "\n",
    "directory = r\"archive\\valid\\Positive\"\n",
    "Grayscaling(directory)\n",
    "directory = r\"archive\\valid\\Negative\"\n",
    "Grayscaling(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c8354",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4585d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_images = os.listdir(r\"archive\\valid\\Negative\")\n",
    "positive_images = os.listdir(r\"archive\\valid\\Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e29ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "image_resize = 224\n",
    "batch_size_training = 50\n",
    "batch_size_validation = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fc2dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf328002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26639 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    r\"archive\\train\\output\",  # Use raw string to avoid escape sequences\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eb969b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = data_generator.flow_from_directory(\n",
    "    r\"archive\\valid\",\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c18e9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bc21e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(ResNet50(\n\u001b[0;32m      3\u001b[0m     include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     ))\n\u001b[1;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[43mnum_classes\u001b[49m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfab77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5af8c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9bd6cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,098</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m4,098\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,591,816</span> (90.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,591,816\u001b[0m (90.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,104</span> (16.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,104\u001b[0m (16.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52f30729",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', F1Score(num_classes=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8fcb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51abe3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m533/533\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 1s/step - accuracy: 0.9983 - f1_score: 0.9983 - loss: 0.1483 - val_accuracy: 0.9900 - val_f1_score: 0.9900 - val_loss: 0.1175\n",
      "Epoch 2/3\n",
      "\u001b[1m533/533\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64us/step - accuracy: 0.0000e+00 - f1_score: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/3\n",
      "\u001b[1m533/533\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m812s\u001b[0m 2s/step - accuracy: 0.9999 - f1_score: 0.9999 - loss: 0.0832 - val_accuracy: 0.9900 - val_f1_score: 0.9900 - val_loss: 0.0809\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=3,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "072e518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_generator_test = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "test_generator = data_generator_test.flow_from_directory(\n",
    "    r'archive/test',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size = batch_size_validation,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c4049f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BPIT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - f1_score: 1.0000 - loss: 0.0574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05762780085206032, 1.0, 1.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6eecb912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('resnet_model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0829e36",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87aca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45d5d89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_vgg \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[0;32m      2\u001b[0m model_vgg\u001b[38;5;241m.\u001b[39madd(VGG16(\n\u001b[0;32m      3\u001b[0m     include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     ))\n\u001b[0;32m      7\u001b[0m model_vgg\u001b[38;5;241m.\u001b[39madd(Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(VGG16(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))\n",
    "model_vgg.add(Dense(num_classes, activation='softmax'))\n",
    "model_vgg.add(Dense(num_classes, activation ='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9460743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59b5adda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m1,026\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,715,720</span> (56.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,715,720\u001b[0m (56.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> (4.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,032\u001b[0m (4.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1dd5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', F1Score(num_classes=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a673d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m283/533\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m9:35\u001b[0m 2s/step - accuracy: 0.5067 - f1_score: 0.5067 - loss: 0.6390"
     ]
    }
   ],
   "source": [
    "fit_history_vgg = model_vgg.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=4,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bb30943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.save('vgg16_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d237779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.evaluate_generator(test_generator,steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed25c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision_val = precision(y_true, y_pred)\n",
    "    recall_val = recall(y_true, y_pred)\n",
    "    f1_val = 2*((precision_val*recall_val)/(precision_val+recall_val+K.epsilon()))\n",
    "    return f1_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c752a942",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#compile the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,recall,precision,f1_score])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',recall,precision,f1_score])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c65d9",
   "metadata": {},
   "source": [
    "### An array of [Loss, Accuracy, Recall, Precision, F1 Score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403573e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(test_generator,steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4b4c9",
   "metadata": {},
   "source": [
    "### Prediction for cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4c09c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"archive\\valid\\Positive\"\n",
    "Bilateral_filtering(directory)\n",
    "Grayscaling(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f06afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='E:\\\\Bali\\\\Kaggle Competition\\\\predict\\\\IMG_1129.JPG') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "08a095e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(\"E:\\\\Bali\\\\Kaggle Competition\\\\predict\\\\IMG_1129.JPG\", target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ea808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('resnet_model1.h5')\n",
    "\n",
    "# Make a prediction\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Decode the predictions\n",
    "# Load the class labels\n",
    "class_labels = [ \"Negative\",\"Positive\"]\n",
    "\n",
    "# Get the index of the class with the highest probability\n",
    "top_class_index = np.argmax(predictions)\n",
    "\n",
    "# Get the class label\n",
    "top_class_label = class_labels[top_class_index]\n",
    "print(top_class_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
